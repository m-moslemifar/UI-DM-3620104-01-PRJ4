{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load train and validate datasets and concatenate them to do preprocessing on whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hazm import *\n",
    "from gensim.models import Word2Vec\n",
    "import codecs\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "t = pd.read_csv('train - small.csv', delimiter='\\t', index_col=0)\n",
    "v = pd.read_csv('dev - small.csv', delimiter='\\t', index_col=0)\n",
    "train = t[['comment', 'label_id']].copy()\n",
    "validate = v[['comment', 'label_id']].copy()\n",
    "frames = [train, validate]\n",
    "data = pd.concat(frames, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert all characters to lowercase"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data['comment'] = data['comment'].apply(lambda x: ' '.join(x.lower() for x in x.split()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load stop words and delete stop words from dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nmz = Normalizer()\n",
    "stops = sorted(\n",
    "    list(set([nmz.normalize(w) for w in codecs.open('stopwords.dat', encoding='utf-8').read().split('\\n') if w]))\n",
    ")\n",
    "\n",
    "data['comment'] = data['comment'].apply(lambda x: ' '.join(x for x in x.split() if x not in stops))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lemmatize words in dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "data['comment'] = data['comment'].apply(lambda x: ' '.join([lemmatizer.lemmatize(x) for x in x.split()]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define an iterator to iterate through sentences and fit Word2Vector model on sentences to get word vectors and word indexes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SentenceIterator:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __iter__(self):\n",
    "        for review in self.dataset.iloc[:, 0]:\n",
    "            for sentence in review.split('.')[:-1]:\n",
    "                words = [w for w in sentence.split(' ') if w != '']\n",
    "                yield words\n",
    "\n",
    "\n",
    "sentences = SentenceIterator(data)\n",
    "w2v_model = Word2Vec(sentences=sentences)\n",
    "w2v_model.train(sentences, epochs=10, total_examples=len(list(sentences)))\n",
    "w2v_weights = w2v_model.wv.vectors\n",
    "vocab_size, embedding_size = w2v_weights.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tokenize words in each sentence to make sentences into integer sequences and define an iterator to iterate through sequences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def word2token(word):\n",
    "    try:\n",
    "        return w2v_model.wv.key_to_index[word]\n",
    "    # If word is not in index return 0. I realize this means that this\n",
    "    # is the same as the word of index 0 (i.e. most frequent word), but 0s\n",
    "    # will be padded later anyway by the embedding layer (which also\n",
    "    # seems dirty, but I couldn't find a better solution right now)\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = max([len(s) for s in list(sentences)])\n",
    "\n",
    "\n",
    "class SequenceIterator:\n",
    "    def __init__(self, dataset, seq_length):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.translator = str.maketrans('', '', string.punctuation + '-')\n",
    "        self.sentiments, self.ccount = np.unique(dataset.label_id, return_counts=True)\n",
    "\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        for comment, label_id in zip(self.dataset.iloc[:, 0], self.dataset.iloc[:, 1]):\n",
    "            words = np.array([word2token(w) for w in comment.split(' ')[:self.seq_length] if w != ''])\n",
    "\n",
    "            yield words, label_id\n",
    "\n",
    "\n",
    "sequences = SequenceIterator(data, MAX_SEQUENCE_LENGTH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split dataset into X and Y and also train and validate datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "set_x = []\n",
    "set_y = []\n",
    "for w, l in sequences:\n",
    "    set_x.append(w)\n",
    "    set_y.append(l)\n",
    "\n",
    "set_x = pad_sequences(set_x, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "set_y = np.array(set_y)\n",
    "\n",
    "x = set_x[:6000]\n",
    "y = set_y[:6000]\n",
    "val_x = set_x[6000:]\n",
    "val_y = set_y[6000:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the neural network model and add the appropriate layers, compile the model and print out the details"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,\n",
    "                    output_dim=embedding_size,\n",
    "                    weights=[w2v_weights],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    mask_zero=True,\n",
    "                    trainable=False))\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fit the model on the train dataset and validate them against the validate dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit(x, y, epochs=5, batch_size=32,\n",
    "                    validation_data=(val_x, val_y), verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot train and validation loss and train and validation accuracy through epochs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Accuracy')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predict sentiments using the model and calculate and print out the F1 score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_predictions = model.predict(x).tolist()\n",
    "validate_predictions = model.predict(val_x).tolist()\n",
    "train_predictions = list(map(lambda x: 0 if x[0] < 0.5 else 1, train_predictions))\n",
    "validate_predictions = list(map(lambda x: 0 if x[0] < 0.5 else 1, validate_predictions))\n",
    "train_standard = train['label_id'].tolist()\n",
    "validate_standard = validate['label_id'].tolist()\n",
    "\n",
    "train_identical = 0\n",
    "train_length = min([len(train_predictions), len(train_standard)])\n",
    "for i in range(train_length):\n",
    "    if train_standard[i] == train_predictions[i]:\n",
    "        train_identical += 1\n",
    "print('Train dataset predictions similarity to source: ' + str(train_identical / train_length))\n",
    "\n",
    "validate_identical = 0\n",
    "validate_length = min([len(validate_predictions), len(validate_standard)])\n",
    "for i in range(validate_length):\n",
    "    if validate_standard[i] == validate_predictions[i]:\n",
    "        validate_identical += 1\n",
    "print('Validate dataset predictions similarity to source: ' + str(validate_identical / validate_length))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}